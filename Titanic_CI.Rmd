---
title: "CI_Project2_Titanic"
author: "GV_AS"
date: "25 December 2015"
output: html_document
---

Predicting Survival on the Titanic

The objective of the competition is to predict whether a passenger would survive or not.One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

Setting your working directory:

```{r}
setwd("C:/Users/shankar/Desktop")
load('.RData')
```

Loading Data 
```{r}
train<-read.csv('train.csv')
test<- read.csv('test.csv')
```

The Training dataset consits of 891 rows and 12 columns.

```{r, echo=FALSE}
colnames(train)
```
Distribution of Sexes among the population
```{r, echo=FALSE}
train$Sex <- as.factor(train$Sex)
table(train$Sex)
```
Rate of survival for Males and Females
```{r, echo=FALSE}
train$Survived <- as.factor(train$Survived)
table(train$Sex, train$Survived)
plot(train$Sex, train$Survived)
```

Females have a better survival rate. 75% of the women have survived, whereas only 18% of men survived.

Distribution of people across classes

```{r, echo=FALSE}
table(train$Pclass)
```
Does the passenger class have an impact on survival?

```{r, echo=FALSE}
train$Pclass <- as.factor(train$Pclass)
table(train$Pclass,train$Survived)
```

```{r, echo=FALSE, warning=FALSE}
train$Survived <- as.factor(train$Survived)
plot(train$Pclass,train$Survived)

total <- train
total$Pclass <- factor(total$Pclass)
levels(total$Pclass) <- c("FirstClass", "SecondClass", "ThirdClass")
total$Survived <- factor(total$Survived)

library(ggplot2)
ggplot(total, aes(Pclass)) + 
  geom_bar(aes(fill = Survived )) + 
  facet_grid(~Sex) + 
  ggtitle("Pclass and Sex as the Survival Factors")


```

Females in the upper classes have a higher rate of survival

```{r, echo=FALSE}
mosaicplot(train$Embarked ~ train$Survived, 
           main="Passenger Fate by Port of Embarkation",
           shade=FALSE, color=TRUE, xlab="Embarked", ylab="Survived")
```


While passenger survival didn't vary as much across the three ports of embarkation as it did between genders and traveling classes, perhaps the Embarked feature will prove useful at some point.

Let's now move on to AGE.

```{r, echo=FALSE}
summary(train$Age)
```

There are 177 NA's in AGE.

Let's do a summary of Fare.
```{r, echo=FALSE}
summary(train$Fare)
```

Zero Fare? 

```{r}
subset(train, Fare < 5, select = c(Age, Fare))
```

some error here, let's substitute the zero fares with the median values of the fares corresponding to each passenger class.

```{r}
aggregate(Fare~Pclass,train,median)
```

Lets combine Train and Test Data , as fix for Age and Fare has to done in both datasets.

```{r}
#Combining the data 
test$Survived<-NA
combi<-rbind(train,test)
```
```{r}
#Extracting the information from Names to predict missing ages
combi$Name <- as.character(combi$Name)
strsplit(combi$Name[1], split='[,.]')
strsplit(combi$Name[1], split='[,.]')[[1]]
strsplit(combi$Name[1], split='[,.]')[[1]][2]
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Mr'
combi$Title[combi$Title %in% c('Col','Dr','Jonkheer','Rev')] <- 'Mr'
combi$Title[combi$Title %in% c('the Countess','Mme','Dona','Lady')] <- 'Mrs'
combi$Title[combi$Title %in% c('Mlle')] <- 'Miss'
combi$Fare[is.na(combi$Fare)] <- mean(combi$Fare, na.rm=TRUE)
combi$FamId<- combi$SibSp+ combi$Parch 

#Predicting NAs of Age

for (i in 1:1309) {
  if(is.na(combi$Age[i])){
    if(combi$Title[i]=="Master"){combi$Age[i]<-sample(1:17,1)}
    else if(combi$Title[i]=="Mr"){combi$Age[i]<-sample(18:50,1)}
    else if(combi$Title[i]=="Miss"){combi$Age[i]<-sample(18:29,1)}
    else if(combi$Title[i]=="Mrs"){combi$Age[i]<-sample(30:60,1)}
    else{combi$Age[i]<-sample(18:60,1)}
  }
}

#Predicting the errors in Fares
for(i in 1:1309){
  if(combi$Fare[i]<7 && combi$Pclass[i]==1){combi$Fare[i]<-60}
  else if(combi$Fare[i]<7 && combi$Pclass[i]==2){combi$Fare[i]<-14}
  else if(combi$Fare[i]<7 && combi$Pclass[i]==3){combi$Fare[i]<-8}
  else{combi$Fare[i]<-combi$Fare[i]}
}

#Splitting back the train and test data
train <- combi[1:891,]
test <- combi[892:1309,]

```
summary of Age.

```{r, echo=FALSE}
summary(train$Age)
```
summary of Fare.

```{r, echo=FALSE}
summary(train$Fare)
```



```{r, echo=FALSE}
library(ggplot2)
qplot(Age, data=train, geom="density", fill=Pclass, alpha=I(.5),
          main="Age distribution by class", xlab="Age",
          ylab="Density")
```


It looks like First class people were generally older than second class and so was the case with second class with third class.

The next step is to split the Train data into a Training Set & a Validation Set

```{r}
dataTrain<-train[1:700,]
dataVal<-train[701:nrow(train),]
```
What the above code means is that the first 700 rows of Train data are loaded into the data frame object "dataTrain" and the remaining rows are loaded into "dataVal", which is going to be the validation set.

```{r, echo=FALSE}
#Factoring the required sets
factor(train$Sex, c("male", "female"), labels = c(1, 0))
factor(test$Sex, c("male", "female"), labels = c(1, 0))
factor(train$Embarked, c("C", "Q","S"), labels = c(1, 2,3))
factor(test$Embarked, c("C", "Q","S"), labels = c(1, 2,3))
```

```{r, warning=FALSE}
#------------Model fitting Starts here-------------------------------#

library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(neuralnet)
library(e1071)
library(caret)
library(randomForest)
library(party)
library(caret)



#Applying Decision trees
fit <- rpart(Survived ~ Sex + Age + FamId + Pclass + Fare , data = dataTrain, method="class")
fancyRpartPlot(fit)

dataVal$pred_dt <- predict(fit, dataVal[,-1], type = "class")
conf_Tree <- confusionMatrix(dataVal$pred_dt, dataVal$Survived)
#Confusion Matrix
tab=with(dataVal,table(True=Survived,Predicted=pred_dt))
print(tab)
#Accuracy
sum(diag(tab)) / sum(tab)






# Random Forests
set.seed(415)
fit2<- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + Fare + Embarked + FamId + Parch + SibSp, data=dataTrain,
                    importance=TRUE, ntree=2000)
varImpPlot(fit2)

dataVal$Pred_rf <- predict(fit2, dataVal)
Conf_RForest <- confusionMatrix(dataVal$Pred_rf, dataVal$Survived) 



#C- forest
set.seed(415)
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + FamId, data = dataTrain, controls=cforest_unbiased(ntree=2000, mtry=3))

Pred_cf <- predict(fit, dataVal, OOB=TRUE, type = "response")
Conf_CForest <- confusionMatrix(Pred_cf, dataVal$Survived)

#Applying Neural network
m <- model.matrix( ~ Survived + Pclass + Sex + Age + SibSp + Parch + Fare, data = dataTrain)
net <- neuralnet(Survived ~ Sexmale + Age + Pclass + SibSp + Parch, data=m, hidden = 10, threshold = 0.1)
plot(net)

dataVal_temp<-subset(dataVal,select=c("Pclass","Sex","Age","SibSp","Parch","Fare"))
factor(dataVal_temp$Sex, c("male", "female"), labels = c(1, 0))
n <- model.matrix( ~ Pclass + Sex + Age + SibSp + Parch + Fare, data = dataVal_temp)
prediction<-compute(net, n[,2:6])

for(i in 1:length(prediction$net.result)){
  if(prediction$net.result[i]>0.6){prediction$net.result[i]<-1}
  else{prediction$net.result[i]<-0}
}
conf_Neuralnet <- confusionMatrix(prediction$net.result, dataVal$Survived)


#SVM modelling
plot(dataTrain$Age, dataTrain$Pclass, xlab="Age", ylab="Pclass", col=ifelse(dataTrain$Survived==1, "red", "blue"))
pairs(~Age+Sex+Pclass+Fare+SibSp+Parch,data=dataTrain, col=ifelse(dataTrain$Survived==1, "red", "blue"))

train_svm<-dataTrain[,c("Age","Sex","Pclass","SibSp","Parch","Survived")]
svm.model<-svm(Survived ~ . , data = train_svm, kernel="radial")

dataVal_svm<-dataVal[,c("Age","Sex","Pclass","SibSp","Parch")]
dataVal_svm$Fare[is.na(dataVal_svm$Fare)] <- mean(dataVal_svm$Fare, na.rm=TRUE)
preds<-predict(svm.model, dataVal_svm)

for(i in 1:length(preds)){
  if(preds[i]>0.5){preds[i]<-1}
  else{preds[i]<-0}
}

conf_Svm  <- confusionMatrix(preds, dataVal$Survived)



```


Now it's time for some "feature engineering"
```{r}
train$Child <- ifelse(train$Age<=18,1,0)
```

So anyone who is less than 18 years are considered to be children. Children are mosty likely to be rescued first - let's check it out.

```{r}
table(train$Child, train$Survived)
```

Looks like it doesn't it - there is a ~50% chance that you survive if you are a child.

```{r}
table(train$Child, train$Survived, by=train$Sex)
```

From here, it's also somewhat evident that female children are more likely to survive.

Creating an indicator for Woman
```{r}
train$Woman <- ifelse(train$Sex=="female",1,0)
```

Creating another indicator for Child or Woman
```{r}
train$CoW <- ifelse(train$Child==1 | train$Woman==1,1,0) 
```

NOW COMING TO THE VARIABLES RELATED TO FAMILY.
```{r}
train$FamilySize <- train$SibSp + train$Parch + 1
train$FamilyCat <- cut(train$FamilySize, c(0,1,4,12))
```

What is the family size of the individual? That's what FamilySize gives you. In the next step, I have split the Family Sizes into 3 groups - those who are travelling alone, those whose family size is 2-4 and the third group of individuals whose family size is greater than 4.

To demonstrate how this works, let's tabulate the FamilyCat and survival
```{r}
table(train$FamilyCat, train$Survived)
```

Individuals travelling alone and those with very high family sizes have a bad survival rate.

Now a variable for tracking down the 3rd class passengers alone - the other two passenger classes had a fairly similar survival rate.

```{r}
train$PC3 <- ifelse(train$Pclass=="3",1,0)
```

I have decided to go with the following independent variables:

Pclass # Passenger class - the proxy for socio-economic status
FamilyCat # Family Category - Family Size=1, Family Size 2-4, Family Size >4
Sex
CoW # Is the passenger a Child or a woman?
Title

I'm loading only these required variables into a new data frame:

```{r}
df_train <- train[,c("Survived","FamilyCat","Pclass","Sex","CoW","Title")]
```

The next step is to split the data into a Training Set & a Validation Set

```{r}
dataTrainfe<-df_train[1:700,]
dataValfe<-df_train[701:nrow(df_train),]
```

What the above code means is that the first 700 rows of data are loaded into the data frame object "dataTrainfe" and the remaining rows are loaded into "dataValfe", which is going to be my validation set.

All my variables are categorical, so I'm converting them to factors.
```{r}
dataTrainfe$Survived <- as.factor(dataTrainfe$Survived)  
dataTrainfe$Sex <- as.factor(dataTrainfe$Sex)
dataTrainfe$Title <- as.factor(dataTrainfe$Title)       
dataTrainfe$Pclass <- as.factor(dataTrainfe$Pclass)
dataTrainfe$CoW <- as.factor(dataTrainfe$CoW)   
dataTrainfe$FamilyCat <- as.factor(dataTrainfe$FamilyCat)
```

It's time to move to the validation set. It is essential that we ensure that the datatypes of the input variables are the same across the training and validation sets.

```{r}
dataValfe$Survived <- as.factor(dataValfe$Survived)
dataValfe$Title <- as.factor(dataValfe$Title)
dataValfe$Sex <- as.factor(dataValfe$Sex)
dataValfe$CoW <- as.factor(dataValfe$CoW)
dataValfe$Pclass <- as.factor(dataValfe$Pclass)
dataValfe$FamilyCat <- as.factor(dataValfe$FamilyCat)  
```


```{r}
#Applying Decision trees
fit <- rpart(Survived~Pclass+Sex+CoW+FamilyCat+Title , data = dataTrainfe, method="class")
fancyRpartPlot(fit)

dataVal$pred_dt <- predict(fit, dataValfe[,-1], type = "class")
conf_Tree <- confusionMatrix(dataValfe$pred_dt, dataValfe$Survived)
#Confusion Matrix
tab=with(dataValfe,table(True=Survived,Predicted=pred_dt))
print(tab)
#Accuracy
sum(diag(tab)) / sum(tab)






# Random Forests
set.seed(415)
fit2<- randomForest(as.factor(Survived) ~ Pclass+Sex+CoW+FamilyCat+Title, data=dataTrainfe,
                    importance=TRUE, ntree=2000)
varImpPlot(fit2)

dataValfe$Pred_rf <- predict(fit2, dataValfe)
Conf_RForest <- confusionMatrix(dataValfe$Pred_rf, dataValfe$Survived) 



#C- forest
set.seed(415)
fit <- cforest(as.factor(Survived) ~ Pclass+Sex+CoW+FamilyCat+Title, data = dataTrainfe, controls=cforest_unbiased(ntree=2000, mtry=3))

Pred_cf <- predict(fit, dataValfe, OOB=TRUE, type = "response")
Conf_CForest <- confusionMatrix(Pred_cf, dataValfe$Survived)
```